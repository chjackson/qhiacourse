---
title: Uncertainty in quantitative health impact modelling
author: Christopher Jackson<br>MRC Biostatistics Unit, University of Cambridge
email: chris.jackson@mrc-bsu.cam.ac.uk
format:
  revealjs: 
    embed-resources: true
    slide-number: true
    incremental: true
    preview-links: auto
    theme: styles.scss
---

<!-- https://github.com/ITHIM/ITHIM-R/tree/simple -->


<!-- 

Map of presentation. 
[min] 1 - 10, what is a parameter, what is uncertainty 
[min] 11 - 20, how to get credible intervals 
[min] 21 - 26, how to get distributions 
[min] 27 - 30, Doing Monte Carlo.  SA 
[min] 32 - 42, VoI 


OPPORTUNITIES for interaction 
* [slide 1] How many of you have done a probabilistic model?
* [slide 1] How many of you have done one-way sensitivity analysis?


TODO 

* How many samples in MC? 

* ITHIM picture

* Work out how to interact on indiv/unc,  unc examples,  whys unc matter 

* UQ practical 

* MC practical 

* VoI practical 

* MC animation?



Paper example 
Scenario where pm2.5 pollution emissions from transport D x less 
Incidence of stroke 
Pars: 

* background conc mu
* prop pi due to transport 
* linear rel between change in emissions and change in conc, 
  background in scen then mu*(pi*D + (1 - pi))
* baseline incidence I0, exp cases per year 
* dose response g2(x,d) for exposure x and pars d 
* rel incidence R = g2(scen conc, d) / g2(base conc, d)
* reduction is I0 - I0*R

Uncs.
Background: lognormal with given CI 
Pi-transport: beta dist 
Relative risk from a statistical model with 

"[pars] estimated from data given by the published relative risks at
different exposures x. A sample from the joint distribution for d was
obtained by repeating this estimation with alternative plausible
relative risks simulated using the published estimates and their
standard errors" 

huh. could use as an example of an adhoc unc calculation 
is it a bootstrap 
more like a meta analysis 
or is it like a deterministic model in itself 
like nonlin pars = g( pub RRs ) and pub RRs are uncertain
g is best fit line 
Thing this excludes is unc about fitted curve given specific RRs 
howd i do it??  like a bayes meta analysis 

Could represent by a multivariate normal rather than sample -- may be clearer??
yeah start there for teaching --- illustrates that pars correlated -- log scale 


--> 




# Uncertainty in HIA models: Summary of lecture

* Parametric models --- definitions

* Quantifying parameter uncertainty using **probability distributions**

* Questions we can answer better by quantifying uncertainty

    - **Uncertainty analysis** --- how confident are our results, how strong is the evidence?
	
	- **Sensitivity analysis** --- how would results change if part of the model changed?

    - **Value of Information** (VoI) analysis --- what parts of the model do we need better evidence for?


# Quantitative models

Models approximate process that generates some output of interest, helping to inform decision-making.

Example: ITHIM health impact model [ TODO borrow picture ]

* **inputs**: travel data, physical activity, air pollution, traffic injuries<br>
              scenarios of change in these

* **outputs**: population health (and changes under scenarios)

. . . 

These generally involve **parameters**.  What are these? 

<!-- ITHIM inputs: 
[city]
* Travel survey individuals
* Mortality from disease : deaths and YLL for a pop [within year]? 
* City population (scaled from country mortality)
[global]
AP DRFs 
PA DRFs. 
--> 


# Parametric models 

Model input parameters: examples

* background exposure, e.g. average levels of an air pollutant, or levels of physical activity [(for some population, area, characteristics)]{#small18}

* relative risk of getting a disease, given some change in some exposure

. . .

Parameters are **representations of knowledge** about a [(real or imagined)]{#small16} population.

* Usually conceive as *summaries* of individual quantities over a large/infinite "population". 

<!-- [ group discussion, exercise, "What is a parameter ] -->

. . .

Model transform parameters $\rightarrow$ output quantities.<br>
Outputs also represent summaries over populations

<!-- Premise of lecture... --> 


# Parameters are generally uncertain

Two broad reasons for parameter uncertainty: the parameter estimate is 

* an observed summary of a **limited population**, and/or

* from a **population that is different from the one we want**

. . .

<!--	- Discuss examples of this  -->
	
<!-- different country. past vs future.  selection bias ---> 

. . .

**Models are also uncertain** [("structural" uncertainty)]{#small16}

* By definition, model approximates reality [("all models are wrong, but some are useful"...)]{#small16}

* Ideally *quantify* as many uncertainties as possible *inside* the model

<!-- 

Exercise or discussion ?   Slido or such? 
baseline AP/PA 
baseline mortality, YLL 
Dose response 
what is the learning objective. there's always uncertainty. 
or is it by definition that a parameter is a summary of the population 
could just reiterate that 

--> 





# Individual variability versus uncertainty about knowledge

| Individual-level quantities  |  Parameters |
| ---  | --- |
| individual exposure to PM2.5 during a trip | expected exposure to PM2.5 for a trip/individual of this kind |
| whether this individual dies within a year | risk of death within a year |

. . .

Individual-level quantities are **different for each individual**

Our **knowledge** of parameters may be **uncertain**

. . . 

**Individual variability can never be removed, but parameter uncertainty can be reduced with better knowledge**

<!-- Exercise to test the difference --> 



# Why does uncertainty matter? 

<!-- [ interact ?  ]  --> 

We've built best model we can.  Just report our "best estimate" to the decision maker? 

* Decision makers need good evidence to change practice --- models should indicate strength of evidence for result

* What about the future - we may be able to get better evidence - what research should be done?

. . .

Here we will cover **quantitative methods** for assessing uncertainty.  In particular, **probabilistic** methods. 

* though there is a broader field of appraising evidence, engaging with stakeholders or experts etc...



## Questions relevant to uncertainty quantification

**Uncertainty analysis**: about _strength of evidence_

* What range of outputs are plausible, given current evidence? 

* Which parameters most influence the output uncertainty?


**Sensitivity analysis**: _"what if..."_

* What if parameter took the value $b$ instead of $a$ --- how would the output change? 


**Value of Information analysis**

* How much would the model **improve** if we got better information?<br>
  Which parameter should we get better information on?




# Uncertainty in microsimulation models 

**(advanced, may remove this slide)**

Microsimulation models create a population of *synthetic* individuals and simulating / summarising quantities of interest

Often based on drawing individuals directly from an *observed* dataset

* [(in contrast with models where inputs and outputs represent population summaries)]{#small18}

. . .

<b>ITHIM:</b> synthetic population of "individuals" (households, people, trips) based on travel survey data.  Exposures assigned to trips.  Health impacts for aggregated age groups. 

::: {.fragment #small18}
Uncertainty here applies to *individual-level* quantities (e.g. trips taken for a person with given characteristics), and is represented by variability between individuals in the survey data.  *This approach doesn't consider uncertainty about quality/relevance of the travel survey dataset*
:::


# How to quantify uncertainty 

Two broad approaches

(a) **Statistical analyses** of data (your own, or published analyses) giving *point* and *interval* estimates or standard errors

(b) **Judgements** (informal or from structured elicitation), e.g.

* *point estimate* (best guess) for parameter value 

* *credible interval* e.g. "I judge that the parameter is between $a$ and $b$, with 95% confidence"

. . . 

Our goal in each case is to obtain **probability distributions** for parameters

* rigorous basis for uncertainty and sensitivity analysis


# Probability distributions 

A **full probability distribution**

:::: {.columns}

::: {.column width="50%"}
![](plots/densarea.svg) 
:::

::: {.column width="50%"}
For any pair of values $(a,b)$, we can deduce the probability that the parameter is between $a$ and $b$.
:::
::::



# Statistical analyses: Bayesian methods

In statistical models, data assumed to come from models with parameters...

. . . 

:::: {.columns}

::: {.column width="55%"}

[e.g. number of disease cases is Binomial (population n, underlying prevalence p)]{#small18}

* **Bayesian methods** based around quantifying parameter uncertainties as probability distributions
  
* **Prior distribution** combined with study data $\rightarrow$ **posterior** distribution.

::: {.fragment #small16}
Prior distribution dominates if data are weak<br>
Prior doesn't matter if enough data 
:::

:::

::: {.column width="45%"}


![](plots/prior_post.svg)

:::

::::




# Statistical analyses: frequentist methods

* Construct an **estimator** of parameters based on a finite dataset

* Uncertainty quantified by imagining different datasets drawn from the same population

    * **Standard error**: variability in estimates between datasets.
	
	* **Confidence interval**: contains true value in 95% of datasets drawn
	
. . .

If dataset is large, can interpret a frequentist analysis as Bayesian:  parameter has a normal distribution, defined by estimate and standard error

If dataset is small, Bayesian analyses have the benefit of allowing background information to be included as a prior 



# Examples of statistical models that inform quantitative HIA

**Global**

* Epidemiological models for estimating disease incidence and mortality

* Meta-analysis of relative risks (of disease outcomes given different exposures)


**Local** 

* Road traffic deaths or injuries: count data (e.g. Poisson) regression models of reported incidents by person/vehicle characteristics

* Models for analysing travel survey data 


# Examples of judgements: physical activity 

MMET value for cycle commuting.  Data from Compendium of Physical Activities [(https://pacompendium.com/bicycling/)]{#small16}

![](plots/met_cycling.png){}

Which of these are relevant to our particular health impact model?



# Examples of judgements: physical activity 

:::: {.columns}

::: {.column width="50%"}
Might judge that code 1011 (MET 6.8) is a typical kind of cycling for our context, and judge a credible interval based on similar kinds of cycling in this table. 

**Rough judgement: no "correct answer"!**

<b>Note:</b> Does the parameter in our model represent the average for some population, or the value for a person / trip?  
:::


::: {.column width="50%"}
![](plots/met_cycling.png){width="100%"}

<b>Note also:</b>
[Each published MMET value is itself an estimate, that conceals variability and uncertainty!]{#small18}

:::
:::: 



# Examples of judgements: air pollution

PM2.5 concentration. We know published estimates of average and SD for 30 cities in India.

:::: {.columns}

::: {.column width="50%"}
Suppose our city is not one of these.  We just know the *average* PM2.5 for our city.  Where do we get a *credible interval* or SD?

::: { #small18}
Indian data shows a "typical" set of SDs of pollution within cities.   So we might use an average of these SDs.  But...

::: {.fragment}

* What do we know about how our city compares to these? 

* Do we want a measure of uncertainty (about an average) or variability (between points)?
:::
:::

:::

::: {.column width="50%"}
![](plots/india_pm25_data.png)
[Source: https://urbanemissions.info/wp-content/uploads/apna/docs/2019-07-APnA30city_summary_report.pdf]{#small16}

:::

::::


# Meta-analysis 


:::: {.columns}

::: {.column width="50%"}

Formally averaging a quantity estimated from different studies.

Give more weight to more confident studies and/or those closer to our context.

Most developed in randomised clinical trials --- highly controlled, regulated studies

::: {#small16}
see e.g. [https://training.cochrane.org/interactivelearning](https://training.cochrane.org/interactivelearning) for learning resources --- find those about observational studies in epidemiology
:::

::: 

::: {.column width="50%" #small18}
![](plots/ma.svg)

Might summarise the average of the observed studies, or make a prediction for a new study (more uncertain)

If studies disagree, consider why.  Ensure relevant studies selected.
:::

::: 


# Considerations for uncertainty quantification 

**Not always possible to precisely quantify uncertainty**

* Rough, clearly-stated judgement better than ignoring it
 
* Uncertainty about a parameter may or may not affect the uncertainty about a model output [(see "Value of Information", later...)]{#small16}

    - If output is robust to uncertainty, conclusion is strengthened

. . .

When making judgements, consider, e.g.: 

* **quality**/amount of data behind published numbers. how were these numbers obtained?

* **relevance** of population, variations over time / between places




# Distributions from credible intervals

:::: {.columns}

Suppose we have a parameter with estimate 0, credible interval (-2 to 2)

How to derive a probability distribution that reflects this belief?

::: {.column width="50%"}
What is wrong with a distribution like this
:::

::: {.column width="50%"}
![](plots/bare_unif.svg)
:::

::::

# Distributions from credible intervals

Suppose we have a parameter with estimate 0, credible interval (-2 to 2)

:::: {.columns}

::: {.column width="50%"}
A triangular distribution is a bit more plausible
:::

::: {.column width="50%"}
![](plots/bare_tri.svg)
:::

::::

# Distributions from credible intervals

Suppose we have a parameter with estimate 0, credible interval (-2 to 2)

:::: {.columns}

::: {.column width="50%"}
A normal distribution is even better
:::

::: {.column width="50%"}
![](plots/bare_norm.svg)
:::

::::



# Normal distribution

:::: {.columns}

::: {.column width="70%"}

Used for quantities with **unrestricted ranges**

Defined by **mean** $\mu$ and standard deviation $\sigma$ (or variance $\sigma^2$)

**95% credible interval is $\pm 2$ SDs: i.e. width is 4 SDs.**: SD easily derived from a CI. 

::: 

::: {.column width="30%"}
![](plots/norm.svg)
::: 

::::

# Log-normal distribution

:::: {.columns}

::: {.column width="70%"}
Used for **positive-valued** quantities.

Normal distribution for the log of the quantity

Example: MMET/h estimate 2.5 (CI 1 to 4).

* Transform to log(MMET).

* Estimate log(2.5), CI (log(1) to log(4)).

* Assign normal with SD = CI width / 4


::: {.fragment #small18} 

If the SD is published, but not the CI<br>
e.g. MMET/h estimate $m=2.5$ (SD $s=1.4$)?

_Method of moments_ gets us mean $\mu$ and SD $\sigma$ on log scale 

[$\mu = \log(m/\sqrt{s^2/m^2 + 1}), \sigma= \sqrt{\log(s^2/m^2 + 1)}$]{#small16}


:::

:::

::: {.column width="30%"}
![](plots/lnorm.svg)
:::

::::




# Beta distribution 

:::: {.columns}

::: {.column width="60%"}
Used for quantities between 0 and 1: probabilities, proportions

Defined by "shape" parameters $a,b$

Given an estimate $m$ and credible interval, how to obtain $a, b$?

::: {.fragment}
Approximate SD $s =$ CI width/4, then use "method of moments":<br>
[$a = (m(1-m)/s^2 - 1)m$]{#small16}</br>
[$b = (m(1-m)/s^2 - 1)(1 - m)$]{#small16}
:::

::: {#bitsmaller}
<!-- Example (a): estimate 0.4, 95% CI 0.2 to 0.6

Example (b): estimate 0.04, 95 %CI 0.01 to 0.4 -->
::: {.fragment}
"SD = CI width / 4" heuristic based on symmetric, normal-like distributions.<br>
Less accurate in example (b) opposite.<br>
Could adjust by trial and error to match desired belief more closely.<br>
:::

:::

:::

::: {.column width="40%"}
![](plots/beta.svg)
[See `SHELF` R package for more sophisticated techniques for fitting distributions]{#small16}
:::

::::




# Practical - quantifying uncertainty around parameters 

[ reference to ITHIM ] 



# Doing uncertainty analysis: Monte Carlo simulation 

For each $i = 1, 2, \ldots N$ (enough to give precise summaries)

1. Simulate parameters $X_i$ from their uncertainty distributions

2. Compute the model output $Y_i = g(X_i)$ 

. . . 

producing a sample from the model outputs $Y_1, \ldots, Y_N$   [ animate? ] 

Summarise the sample to give e.g. 

* a credible interval for the outputs

* probability that e.g. number of deaths $>$ [important value] 

<!-- 
https://medium.com/@norisk/visualizing-monte-carlo-simulations-with-r-and-gganimate-97b4237925f7
Do this with a trivial model eg the voi hia 
--->




# Technical point - uncertainty affects "best estimate" in nonlinear models

Given a model with uncertain inputs $X$, with expected values $E(X)$<br>
Outputs $Y = g(X)$ [where $g()$ is function that defines the model]{#small18}

. . .

What is $E(Y)$, the expected value of $Y$?  

. . .

Can we just plug in the best estimates of the inputs, as $g(E(X))$? 

. . .


$E(Y)$ only equals $g(E(X))$ if the function $g()$ is **linear**:

::: {.callout-tip title="Proof" appearance="minimal"}
$g(E(x)) = g((X_1 + \ldots + X_n)/n)$, then if $g()$ is linear, this equals <br>
$(g(X_1) + \ldots + g(X_n)) / n = E(g(X)) = E(Y)$.
:::

Most realistic models are **non-linear**, therefore need Monte Carlo simulation to get the true expectation of the output. 


# One-way sensitivity analysis {#small22}

Answers questions like "what if [parameter] was actually $b$ instead of $a$"

:::: {.columns}

::: {.column width="50%"}
For each parameter, compare model output with 

* parameter at a "low value"

* parameter at a "high value" 

::: {.fragment}

What if all the _other_ parameters are uncertain?  **Probabilistic one-way sensitivity analysis**: 

* Fix [parameter] at high or low value

* Run the model under Monte Carlo analysis, using the uncertainty distributions of the other parameters

:::

:::

::: {.column width="50%"}

Tornado plot

![](plots/tornado.svg)

:::

::::



# Limitations of one-way sensitivity analysis 

Arbitrary choice of "low" and "high" value may not convey the effect of the parameter

:::: {.columns}
::: {.column width="50%"}
   e.g. if effect of parameter is nonlinear
:::
::: {.column width="50%"}
![](plots/sa_nonlin.svg)
:::
:::: 

. . .

What if parameters have correlated effect on the outcome?<br>
Hard to vary more than one parameter at a time 





# Practical - uncertainty analysis in a model


[ reference to ITHIM ] 



# Value of Information 

Recall

* <b>sensitivity analysis:</b> _"what if model was a bit different"_

* <b>uncertainty analysis</b>: _"what is strength of evidence in model"_

. . . 

A different (related) question: _"what would be the benefit of getting better information"_.

This is **Value of Information** analysis



# Value of Information 

Given current information, model output is uncertain.  But how much more precise would it get... 

if we were to learn some parameter exactly?

* **Expected value of partial perfect information**

. . .

if we conducted a study [(say, a survey of 100 people)]{#small16} to estimate it

* **Expected value of sample information**

. . . 

Helps us to:<br>
**set research priorities** to reduce uncertainty<br>
**design studies** (more advanced, not covered here)


<!-- said later, could delete
<b>Note</b>: we don't know the parameter exactly, or the result of the study, when we calculate this --- so we calculate an _expected_ value, given what is currently known
--->



# Value of Information in terms of health economics

Value of Information methods developed in **health economics** 

* Model a health policy decision and its consequences (e.g. health benefits as QALYs vs costs).   Parameter uncertainties as probability distributions 

* <b>Information has value</b>: $\rightarrow$ reduces parameter uncertainty $\rightarrow$ more precise model outputs $\rightarrow$ better informed policy-making $\rightarrow$ health benefits

. . .

Not previously used much in **health impact modelling**, where models used more for *scenarios* than *policies*

How can we define "value" if we don't model a health policy? 


# Precision as value

Define value as **"precision of estimate"** (e.g. health impact of scenario)

* *How much will the variance (or SD, or credible interval width) be expected to reduce if we got better information?*

* More precise estimates (implicitly) assumed to ultimately lead to benefits

* If needed, trade off informally with costs of research [(willingness to pay for more precise estimates? Not covered here)]{#small16}

. . . 

How exactly to do these computations?


# Computing expected value of partial perfect information (EVPPI)

Model $Y = g(X_1, X_2,.. )$ with some (scalar) output $Y$ and multiple inputs $X_r$

Do uncertainty analysis: define distributions for each $X_r$, obtain distribution for $Y$ via Monte Carlo simulation.

$var(Y)$: variance of model output under *current information*

. . . 

**Definition of EVPPI** - *expected* reduction in this variance if we were to learn the *exact value* of $X_r$ (say)

$$var(Y) - E_x(var(Y | X_r = x))$$

::: {#bitsmaller}

We don't know the value of $X_r$ when we calculate this --- so we must take the **expectation** over possible values $x$ (using the distribution we defined)

:::


# Illustration of EVPPI computation

:::: {.columns}

::: {.column width="55%"}
Take the Monte Carlo sample, with all parameters uncertain

Regression of $Y$ = model output versus $X$ = parameter of interest 

**Expected reduction in variance of $Y$ if we learnt $X$**

$$var(Y) - E_x(var(Y | X = x))$$

::: {#small16}
We don't know $X$ so we average over our uncertainty
:::

:::

::: {.column width="45%"}
![](plots/evppi_scatter.svg)


::: {#small16}
$var(Y)$: uncertainty before learning $X$

$E_x(var(Y | X = x))$: *residual variance*: mean of ("observed" - fitted) over different $x$
:::


:::

::::


# Spline regression for computing EVPPI 

Regression function of output on input will not necessarily be linear.

*Spline* regression [(automated choice of smoothness, works well enough in this context)]{#small16}

```
lm(Y ~ X)  # linear model
library(mgcv)
gam(Y ~ X) # spline ("generalized additive") model
```

The `voi` package can take care of extracting ingredients needed for the 
EVPPI computation (see the practical)

```
library(voi)
evppivar(outputs, inputs)
```

# EVPPI for more than one parameter at once 

**Example**: dose-response curve governed by four different parameters $\alpha$, $\beta$, $\gamma$, $\tau$. Estimate the expected value of *jointly* learning all four of them [(hence learning the dose-response curve)]{#small16}

Regression model with four predictors, e.g.

```
gam(Y ~ alpha + beta + gamma + tau) 
```

More advanced regression models available, which automatically choose the best fitting regression function for Y given the predictors [(and their interactions)]{#small16}: see the practical about the `voi` package


# Presenting EVPPI results

Impact of different parameters on the output uncertainty, presented as 

:::: {.columns}

::: {.column width="55%"}

**Uncertainty that would remain in the output $Y$ if we knew $X$**

* as a variance, $var(Y) - EVPPI_X$

* ...or standard deviation (square root of this)

* ...or rough **credible interval** ($\pm 2$ remaining standard deviation)


::: {.fragment}
or proportion of uncertainty (variance) in $Y$ explained by $X$ 
:::

:::

::: {.column width="45%"}
![](plots/evppi_results.svg)
:::

::::


# Value of Information: further topics 

We will never get perfect information, but we may get **better** information via a new research study.

**Expected value of sample information** is the expected value of a study of a particular design / sample size

* Slightly harder to define and compute

* Previously used in health economic decision models, but not (yet!) used in health impact models

* See `voi` package and references there for some examples.


# Value of Information: further topics 

Here we have just talked about "value" as **variance of outputs**

Value of Information also used widely for health economic **decision** models

* Model chooses policy with the highest net benefit (or lowest loss)
   
* "Value" is the net benefit (NB) itself
   
* Value of information is NB(better information) - NB(current information)

. . .

See `voi` package and references there for some examples.



# Practical session 

... Use of `voi` R package





# Discussion

What uncertainty/sensitivity questions do you want to answer in your own work? 

Do you have the tools to do this? 

