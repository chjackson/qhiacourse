---
title: Uncertainty in quantitative health impact modelling
author: Christopher Jackson<br>MRC Biostatistics Unit, University of Cambridge
email: chris.jackson@mrc-bsu.cam.ac.uk
format:
  revealjs: 
    embed-resources: true
    slide-number: true
    incremental: true
    preview-links: auto
    theme: styles.scss
---

# !!WORK IN PROGRESS!! 

[ Todo interleave this all with practical sessions ] 

<!-- https://github.com/ITHIM/ITHIM-R/tree/simple -->


# Uncertainty in HIA models: Summary of lecture

* Parametric models --- definitions

* Quantifying parameter uncertainty using _probability distributions_

* Questions we can answer better by quantifying uncertainty

    - _Uncertainty_ and _sensitivity analysis_ of model outputs

    - _Value of Information_ (VoI) analysis


# Quantitative models

Models approximate process that generates some output of interest, helping to inform decision-making.

Example: ITHIM health impact model [ TODO picture ]

* inputs: travel data, air pollution, physical activity, injuries, + scenarios of change

* outputs: population health (and changes under scenarios)

. . . 

These generally involve **parameters**.  What are these? 

<!-- ITHIM inputs: 
[city]
* Travel survey individuals
* Mortality from disease : deaths and YLL for a pop [within year]? 
* City population (scaled from country mortality)
[global]
AP DRFs 
PA DRFs. 
--> 


# Parametric models 

Model input parameters: examples

* background exposure [e.g. PM2.5, PA] in an area

* relative risk for [health outcome] given [change in PA/AP exposure]

. . .

These are **representations of knowledge** about a population.

We can usually conceive parameters as *summaries* of individual quantities over a large/infinite "population". 

<!-- [ group discussion, exercise, describe the population??  ?? ] 

ans: 
* exposure is exposure for an individual of given kind over time and space - actual exposures will vary 

* (number of disease cases in a population with exposure 1 / size of this population) / 
* (number of disease cases in a population with exposure 1 / size of this population)

-->

Parameters $\rightarrow$ model outputs:  

* In public health, usually interested in summaries over a population (or contrasts between sub-populations)


<!-- Premise of lecture... --> 

# Parameter uncertainty 

Knowledge is often uncertain $\rightarrow$ 

* parameters are uncertain 

* conclusions from models are uncertain 


# Individual variability versus uncertainty about knowledge

Example parameters, for some population

* Background exposure to PM2.5.  

* Risk of death within one year.

. . .

We may be sure about the values of these.  Even so...

* Exact exposures will be different for each individual

* Some individuals will die within a year, some won't 

. . .

But we may be uncertain about the parameter values.

. . . 

**Individual variability can never be removed, but parameter uncertainty can be reduced with better knowledge**


<!-- Exercise to test the difference --> 


# Parameters are generally uncertain

Two broad reasons for parameter uncertainty

(a) summary of a limited population

(b) population is different from the one we want 

	- Discuss examples of this 
	
<!-- different country. past vs future.  selection bias ---> 

. . .

Models are also uncertain ("structural" uncertainty)

* "all models are wrong, but some are useful"

* we won't go into this - ideally add more parameters for uncertain things - see briefly later

<!-- 

Exercise or discussion ?   Slido or such? 
baseline AP/PA 
baseline mortality, YLL 
Dose response 
what is the learning objective. there's always uncertainty. 
or is it by definition that a parameter is a summary of the population 
could just reiterate that 

--> 



# Why does uncertainty matter? 

[ interact ?  ] 

We've built best model we can.  Just report our "best estimate" to the decision maker? 

* Decision makers need good evidence to change practice --- models should indicate strength of evidence for result

* What about the future - we may be able to get better evidence - what research should be done?

. . .

Here we will cover **quantitative methods** for assessing uncertainty.  In particular, **probabilistic** methods. 

* though there is a broader field of appraising evidence, engaging with stakeholders or experts etc...



## Questions relevant to uncertainty quantification

**Uncertainty analysis**: about _strength of evidence_

* What range of outputs are plausible, given current evidence? 

* Which parameters are most uncertain (most influence the uncertainty in the outputs)


**Sensitivity analysis**: _"what if..."_

* What if parameter took the value $b$ instead of $a$ --- how would the output change? 


**Value of Information analysis**

* How much would the model **improve** if we got better information?
on some parameter?


# How to quantify uncertainty 

Two broad approaches

(a) Statistical analyses of data (your own, or published analyses) giving **point** and **interval** estimates or standard errors

. . .

(b) Judgements (informal or from structured elictation), e.g.

* **point estimate** (best guess) for parameter value 

* **credible interval** e.g. "I judge that the parameter is between $a$ and $b$, with 95% confidence"

. . . 

Our goal in each case is to obtain **probability distributions** for parameters

* powerful tool for uncertainty and sensitivity analysis


# Probability distributions 

A **full probability distribution**

![](plots/densarea.png)

For any pair of values $(a,b)$, we can deduce probability(parameter between $a$ and $b$) 


# Statistical analyses: Bayesian methods

Statistical analyses of observed data can be Bayesian or frequentist

In either, data assumed to come from models with parameters... 

* e.g. number of injuries in (area,time) from Poisson with rate $\lambda$.

. . . 

* **Bayesian methods** based around quantifying parameter uncertainty with probability. 

* **Prior distribution** combined with study data $\rightarrow$ **posterior** distribution.

[picture] 

. . .

Prior distribution dominates if data are weak.  No data: rely on prior judgement 


# Statistical analyses: frequentist methods

* Construct "estimators" of parameters.

* Uncertainty quantified by imagining repeated samples from a population

    * "Standard error": SD of estimates over repeated sampling.
	* 95% confidence interval: contains true value in 95% of repeated samples.

* Agrees with Bayesian estimates (posterior mean, standard deviation, credible interval) if prior is weak and data are large.
  
* Normal(mean, standard error).  positive on log scale 

. . .

If data are weak, Bayesian methods more helpful, since they allow background information as the prior 



# Examples of judgements

[ We need ITHIM detail here ] 


# Distributions from credible intervals

What is wrong with a distribution like this  [ PICTURE ] 

This is a bit better [ TRIANGULAR ] 

But this kind of thing is most realistic [ NORMAL ]


Different distributions for quantities with different ranges... 


# Normal distribution

95 percent credible interval equals mean pm 2 SDs

Width of CI is four standard deviations

Symmetry. 

Transform  log quantity then put normal on it 

[ interactive shiny apps here ??? ] 


# Beta distribution 

Two parameters - many ways to do it 

Easiest to elicit 4 SD and derive SD.  Works if symmetric 

Or moment matching - demo software.   Example for skewed/lo/hi

Revise and respecify limits if unsure.  Mean vs mode, median.   Limits may not be certain


# Practical - quantifying uncertainty around parameters 

[ reference to ITHIM ] 



# Doing uncertainty analysis: Monte Carlo simulation 

For each $i = 1, 2, \ldots N$ (enough to give precise summaries)

1. Simulate parameters $X_i$ from their uncertainty distributions

2. Compute the model output $Y_i = g(X_i)$ 

producing a sample from the model outputs $Y_1, \ldots, Y_N$   [ animate? ] 

Summarise the sample to give e.g. 

* a credible interval for the outputs

* probability that e.g. number of deaths $>$ [important value] 


# Technical point - uncertainty affects "best estimate" in nonlinear models

Given a model with inputs $X$ and outputs $Y = g(X)$, where $g()$ is some mathematical function.

Inputs $X$ are uncertain, with expected values $E(X)$.

What is the expected value of $Y$?  Can we just plug in the best
estimates of the inputs $g(E(X)$? 

$E(Y)$ is only equal to $g(E(x))$ if the function $g()$ is **linear**:

$$g(E(x)) = g((X_1 + \ldots + X_n)/n) = $$ (if $g()$ linear...)

$$(g(X_1) + \ldots + g(X_n)) / n = E(g(X)) = E(Y)$$ 

Most realistic models are **non-linear*: need Monte Carlo simulation to get the true expectation of the output. 


# One-way sensitivity analysis: probabilistic view

In standard "tornado plots" we vary one parameter at a time 



![](plots/tornado.png)



# Practical - uncertainty analysis in a model


[ reference to ITHIM ] 



# Value of Information 

Recall  

* sensitivity analysis: "what if model was a bit different" 

* uncertainty analysis: "what is strength of evidence in model" 

. . . 

A different (related) question: "what would be the benefit of getting better information". 

**Value of Information** analysis

[Example - connect to a result from probabilistic 1 way SA.  Looks like varying ? within plausible range has more of an effect than... so we might want to get better info ] 


# Value of Information 

How much more precise would x get... 

if we were to learn some parameter exactly

* Expected value of partial perfect information

if we designed a study of ?? people

* Expected value of sample perfect information

. . . 

Key point: we don't know the parameter exactly, or the result of the study, when we calculate this --- so we calculate an _expected_ value, given what is currently known

Helps us to **set research priorities** to reduce uncertainty, and **design studies** (more advanced)


# Value of Information in terms of health economics

Value of Information analysis has mainly been used in **health economics** where we model a health policy decision and its consequences e.g. health benefits as QALYs

* Information has value $\rightarrow$ reduces uncertainty $\rightarrow$ better informed policy making $\rightarrow$ health benefits

* Used a lot in "health technology assessment", seldom in health impact modelling, where models used more for scenarios than policies

. . . 

So why talk about "value", if we don't model the policy? 


# Precision as value  

Define value as "precision of estimate" 

* More precise estimates assumed to ultimately lead to benefits

* Technical benefit: we can use tools to compute VoI developed for health economic decision modelling 

* We can describe precision benefits of extra information, though hard to trade off formally with costs of research (willingness to pay for more precise estimates?)



# Expected value of partial perfect information

Also called the "main effect" in computer model sensitivity analysis literature

Formal definition - expected reduction in variance

Illustration with a scatterplot 


# Computing the EVPPI 

Regression modelling 

Splines (black box) 


# EVPPI for more than one parameter at once 

Multiple regression models

Recommend "earth" method


# Presenting conclusions of EVPPI analysis 

Tornado-like plots, with grouped parameters 


# Practical session 

... Use of "voi" package



# Advanced topics 

Pointers to resources to learn more about 

* VoI in decision models [ book but HTA-focused ] 

* Expected value of sample information [ note done in decision models, see book, but not HIA. ]. 

  Simple examples in paper and package 



# Group discussion of advanced topics 

What uncertainty/sensitivity questions do you want to answer in your own work? 

Do you have the tools to do this? 

