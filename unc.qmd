---
title: Uncertainty in quantitative health impact modelling
author: Christopher Jackson<br>MRC Biostatistics Unit, University of Cambridge
email: chris.jackson@mrc-bsu.cam.ac.uk
format:
  revealjs: 
    embed-resources: true
    slide-number: true
    incremental: true
    preview-links: auto
    theme: styles.scss
---

# !!WORK IN PROGRESS!! 

[ Todo interleave this all with practical sessions ] 

<!-- https://github.com/ITHIM/ITHIM-R/tree/simple -->


<!-- 

Paper example 
Scenario where pm2.5 pollution emissions from transport D x less 
Incidence of stroke 
Pars: 

* background conc mu
* prop pi due to transport 
* linear rel between change in emissions and change in conc, 
  background in scen then mu*(pi*D + (1 - pi))
* baseline incidence I0, exp cases per year 
* dose response g2(x,d) for exposure x and pars d 
* rel incidence R = g2(scen conc, d) / g2(base conc, d)
* reduction is I0 - I0*R

Uncs.
Background: lognormal with given CI 
Pi-transport: beta dist 
Relative risk from a statistical model with 

"[pars] estimated from data given by the published relative risks at
different exposures x. A sample from the joint distribution for d was
obtained by repeating this estimation with alternative plausible
relative risks simulated using the published estimates and their
standard errors" 

huh. could use as an example of an adhoc unc calculation 
is it a bootstrap 
more like a meta analysis 
or is it like a deterministic model in itself 
like nonlin pars = g( pub RRs ) and pub RRs are uncertain
g is best fit line 
Thing this excludes is unc about fitted curve given specific RRs 
howd i do it??  like a bayes meta analysis 

Could represent by a multivariate normal rather than sample -- may be clearer??
yeah start there for teaching --- illustrates that pars correlated -- log scale 


--> 




# Uncertainty in HIA models: Summary of lecture

* Parametric models --- definitions

* Quantifying parameter uncertainty using _probability distributions_

* Questions we can answer better by quantifying uncertainty

    - _Uncertainty_ and _sensitivity analysis_ of model outputs

    - _Value of Information_ (VoI) analysis


# Quantitative models

Models approximate process that generates some output of interest, helping to inform decision-making.

Example: ITHIM health impact model [ TODO borrow picture ]

* inputs: travel data, air pollution, physical activity, injuries, + scenarios of change

* outputs: population health (and changes under scenarios)

. . . 

These generally involve **parameters**.  What are these? 

<!-- ITHIM inputs: 
[city]
* Travel survey individuals
* Mortality from disease : deaths and YLL for a pop [within year]? 
* City population (scaled from country mortality)
[global]
AP DRFs 
PA DRFs. 
--> 


# Parametric models 

Model input parameters: examples

* background exposure [e.g. PM2.5, PA] in an area

* relative risk for [health outcome] given [change in PA/AP exposure]

. . .

These are **representations of knowledge** about a <small>(real or imagined)</small> population.

We can usually conceive parameters as *summaries* of individual quantities over a large/infinite "population". 

<!-- [ group discussion, exercise, describe the population??  ?? ] 

Ans: 

* exposure is exposure for an individual of given kind over time and space - actual exposures will vary 

* (number of disease cases in a population with exposure 1 / size of this population) / 
* (number of disease cases in a population with exposure 1 / size of this population)

Any teaching material from Gelman?  G+H?

-->

Model input parameters $\rightarrow$ model output quantities. In public health, model outputs are also usually summaries over populations


<!-- Premise of lecture... --> 

# Parameter uncertainty 

Knowledge is often uncertain $\rightarrow$ 

* parameters are uncertain 

* conclusions from models are uncertain 


# Individual variability versus uncertainty about knowledge

Example parameters, for some population

* Background exposure to PM2.5.

* Risk of death within one year.

. . .

We may be sure about the values of these.  Even so...

* Exact exposures will be different for each individual

* Some individuals will die within a year, some won't 

. . .

But we may be uncertain about the parameter values.

. . . 

**Individual variability can never be removed, but parameter uncertainty can be reduced with better knowledge**


<!-- Exercise to test the difference --> 


# Parameters are generally uncertain

Two broad reasons for parameter uncertainty

(a) summary of a limited population

(b) population is different from the one we want 

	- Discuss examples of this 
	
<!-- different country. past vs future.  selection bias ---> 

. . .

Models are also uncertain ("structural" uncertainty)

* "all models are wrong, but some are useful"

* we won't go into this - ideally add more parameters for uncertain things - see briefly later

<!-- 

Exercise or discussion ?   Slido or such? 
baseline AP/PA 
baseline mortality, YLL 
Dose response 
what is the learning objective. there's always uncertainty. 
or is it by definition that a parameter is a summary of the population 
could just reiterate that 

--> 



# Why does uncertainty matter? 

[ interact ?  ] 

We've built best model we can.  Just report our "best estimate" to the decision maker? 

* Decision makers need good evidence to change practice --- models should indicate strength of evidence for result

* What about the future - we may be able to get better evidence - what research should be done?

. . .

Here we will cover **quantitative methods** for assessing uncertainty.  In particular, **probabilistic** methods. 

* though there is a broader field of appraising evidence, engaging with stakeholders or experts etc...



## Questions relevant to uncertainty quantification

**Uncertainty analysis**: about _strength of evidence_

* What range of outputs are plausible, given current evidence? 

* Which parameters are most uncertain (most influence the uncertainty in the outputs)


**Sensitivity analysis**: _"what if..."_

* What if parameter took the value $b$ instead of $a$ --- how would the output change? 


**Value of Information analysis**

* How much would the model **improve** if we got better information?
on some parameter?


# How to quantify uncertainty 

Two broad approaches

(a) Statistical analyses of data (your own, or published analyses) giving **point** and **interval** estimates or standard errors

. . .

(b) Judgements (informal or from structured elictation), e.g.

* **point estimate** (best guess) for parameter value 

* **credible interval** e.g. "I judge that the parameter is between $a$ and $b$, with 95% confidence"

. . . 

Our goal in each case is to obtain **probability distributions** for parameters

* powerful tool for uncertainty and sensitivity analysis


# Probability distributions 

A **full probability distribution**

![](plots/densarea.png)

For any pair of values $(a,b)$, we can deduce probability(parameter between $a$ and $b$) 


# Statistical analyses: Bayesian methods

Statistical analyses of observed data can be Bayesian or frequentist

In either, data assumed to come from models with parameters... 

* e.g. number of injuries in (area,time) from Poisson with rate $\lambda$.

. . . 

* **Bayesian methods** based around quantifying parameter uncertainty with probability. 

* **Prior distribution** combined with study data $\rightarrow$ **posterior** distribution.

[picture] 

. . .

Prior distribution dominates if data are weak.  No data: rely on prior judgement 


# Statistical analyses: frequentist methods

* Construct "estimators" of parameters.

* Uncertainty quantified by imagining repeated samples from a population

    * "Standard error": SD of estimates over repeated sampling.
	* 95% confidence interval: contains true value in 95% of repeated samples.

* Agrees with Bayesian estimates (posterior mean, standard deviation, credible interval) if prior is weak and data are large.
  
* Normal(mean, standard error).  positive on log scale 

. . .

If data are weak, Bayesian methods more helpful, since they allow background information as the prior 



# Examples of judgements

[ We need ITHIM detail here ] 


# Distributions from credible intervals

What is wrong with a distribution like this  [ PICTURE ] 

This is a bit better [ TRIANGULAR ] 

But this kind of thing is most realistic [ NORMAL ]


Different distributions for quantities with different ranges... 


# Normal distribution

95 percent credible interval equals mean pm 2 SDs

Width of CI is four standard deviations

Symmetry. 

Transform  log quantity then put normal on it 

[ interactive shiny apps here ??? ] 


# Beta distribution 

Two parameters - many ways to do it 

Easiest to elicit 4 SD and derive SD.  Works if symmetric 

Or moment matching - demo software.   Example for skewed/lo/hi

Revise and respecify limits if unsure.  Mean vs mode, median.   Limits may not be certain


# Practical - quantifying uncertainty around parameters 

[ reference to ITHIM ] 



# Doing uncertainty analysis: Monte Carlo simulation 

For each $i = 1, 2, \ldots N$ (enough to give precise summaries)

1. Simulate parameters $X_i$ from their uncertainty distributions

2. Compute the model output $Y_i = g(X_i)$ 

producing a sample from the model outputs $Y_1, \ldots, Y_N$   [ animate? ] 

Summarise the sample to give e.g. 

* a credible interval for the outputs

* probability that e.g. number of deaths $>$ [important value] 


# Technical point - uncertainty affects "best estimate" in nonlinear models

Given a model with inputs $X$ and outputs $Y = g(X)$, where $g()$ is some mathematical function.

Inputs $X$ are uncertain, with expected values $E(X)$.

What is the expected value of $Y$?  Can we just plug in the best
estimates of the inputs $g(E(X)$? 

$E(Y)$ is only equal to $g(E(x))$ if the function $g()$ is **linear**:

$$g(E(x)) = g((X_1 + \ldots + X_n)/n) = $$ (if $g()$ linear...)

$$(g(X_1) + \ldots + g(X_n)) / n = E(g(X)) = E(Y)$$ 

Most realistic models are **non-linear*: need Monte Carlo simulation to get the true expectation of the output. 


# One-way sensitivity analysis: probabilistic view

In standard "tornado plots" we vary one parameter at a time 



![](plots/tornado.png)



# Practical - uncertainty analysis in a model


[ reference to ITHIM ] 



# Value of Information 

Recall

* sensitivity analysis: "what if model was a bit different" 

* uncertainty analysis: "what is strength of evidence in model" 

. . . 

A different (related) question: "what would be the benefit of getting better information". 

**Value of Information** analysis

[Example - connect to a result from probabilistic 1 way SA.  Looks like varying ? within plausible range has more of an effect than... so we might want to get better info ] 


# Value of Information 

How much more precise would x get... 

if we were to learn some parameter exactly

* Expected value of partial perfect information

if we designed a study of ?? people

* Expected value of sample perfect information

. . . 

Key point: we don't know the parameter exactly, or the result of the study, when we calculate this --- so we calculate an _expected_ value, given what is currently known

Helps us to **set research priorities** to reduce uncertainty, and **design studies** (more advanced)


# Value of Information in terms of health economics

Value of Information analysis has mainly been used in **health economics** where we model a health policy decision and its consequences e.g. health benefits as QALYs

* Information has value $\rightarrow$ reduces uncertainty $\rightarrow$ better informed policy making $\rightarrow$ health benefits

* Used a lot in "health technology assessment", seldom in health impact modelling, where models used more for scenarios than policies

. . . 

So why talk about "value", if we don't model the policy? 


# Precision as value

Define value as "precision of estimate" 

* More precise estimates assumed to ultimately lead to benefits

* Technical benefit: we can use tools to compute VoI developed for health economic decision modelling 

* We can describe precision benefits of extra information, though hard to trade off formally with costs of research (willingness to pay for more precise estimates?)



# Expected value of partial perfect information (value = precision)

Model $Y = g(X_1, X_2,.. )$ with some (scalar) output $Y$ and multiple inputs $X_r$

We do uncertainty analysis: define distributions for each $X_r$, obtain distribution for $Y$ via Monte Carlo simulation.

$var(Y)$: variance of model output under uncertainty

**Definition** - expected reduction in this variance if we were to learn the exact value of $X_r$ (say)

$$var(Y) - E_x(var(Y | X_r = x))$$

We don't know the value of $X_r$ when we calculate this --- so we must take the **expectation** over possible values $x$ (using the distribution we defined)


# Illustration of EVPPI



# Computing the EVPPI 

Regression modelling 

Splines (black box) 


# EVPPI for more than one parameter at once 

Multiple regression models

Recommend "earth" method


# Presenting conclusions of EVPPI analysis 

Tornado-like plots, with grouped parameters 


# Practical session 

... Use of "voi" package



# Advanced topics 

Pointers to resources to learn more about 

* VoI in decision models [ book but HTA-focused ] 


<!-- 
**General definition**

Model is used to make a decision 

Expected (over current knowledge) of
(net benefit with current information - 
net benefit if learn the value of a parameter)
--> 


* Expected value of sample information [ note done in decision models, see book, but not HIA. ]. 

  Simple examples in paper and package 



# Group discussion of advanced topics 

What uncertainty/sensitivity questions do you want to answer in your own work? 

Do you have the tools to do this? 

